use xsk_rs::{
    config::{SocketConfig, UmemConfig},
    umem::Umem,
    socket::Socket,
};
use anyhow::{Result, Context};
#[cfg(target_os = "linux")]
use aya::maps::XskMap;
use std::num::NonZeroU32;
use std::os::unix::io::AsRawFd;

pub struct AfXdpSocket {
    pub tx_q: xsk_rs::socket::TxQueue,
    pub rx_q: xsk_rs::socket::RxQueue,
    pub fq: xsk_rs::umem::FillQueue,
    pub cq: xsk_rs::umem::CompQueue,
    pub umem: Umem,
    pub tx_frames: Vec<xsk_rs::umem::frame::FrameDesc>,
}

impl AfXdpSocket {
    #[cfg(target_os = "linux")]
    pub fn new(ifname: &str, queue_id: u32, xsk_map: &mut XskMap<aya::maps::MapData>) -> Result<Self> {
        // 1. Configure UMEM and Socket with optimized settings
        let umem_config = UmemConfig::default();

        let socket_config = SocketConfig::default();

        let frames_count = NonZeroU32::new(8192).unwrap(); // Increased frame count for better batching

        // xsk-rs 0.8.x direct construction
        let (umem, frame_descs) = Umem::new(umem_config, frames_count, false)
            .context("Failed to build AF_XDP Umem")?;

        // Socket::new(config, &umem, &if_name, queue_id)
        let (tx_q, rx_q, fq_cq) = unsafe {
            Socket::new(
                socket_config,
                &umem,
                &ifname.parse().context("Invalid ifname")?,
                queue_id,
            )
        }.context("Failed to build AF_XDP Socket")?;

        let (mut fq, cq) = fq_cq.context("Fill/Comp queues missing")?;
        
        // 2. Register socket FD in eBPF XSK_MAP
        let fd = tx_q.fd().as_raw_fd();
        
        xsk_map.set(queue_id, fd, 0)
            .context("Failed to register AF_XDP FD in BPF map")?;

        // 3. Initial Fill Queue seeding
        let mut frame_descs = frame_descs;
        let tx_frames = frame_descs.split_off(2048);
        
        unsafe { fq.produce(&frame_descs) };

        log::info!("ðŸš€ AF_XDP Zero-Copy enabled on {} (RX: {}, TX: {})", ifname, frame_descs.len(), tx_frames.len());
        
        Ok(Self { tx_q, rx_q, fq, cq, umem, tx_frames })
    }

    /// Optimized batch processing with memory alignment and DMA hints
    /// Process packets from the RX queue in optimized batches for maximum throughput
    pub fn recv_batch_optimized<F>(&mut self, max_count: usize, mut f: F) -> Result<usize>
    where F: FnMut(&[u8])
    {
        use xsk_rs::umem::frame::FrameDesc;

        // Use larger batch size for better throughput (up to 32 packets)
        let mut descs = [FrameDesc::default(); 32];
        let count_to_consume = max_count.min(32);

        // 1. Consume from RX queue with optimized batch size
        let count = unsafe { self.rx_q.consume(&mut descs[..count_to_consume]) };
        if count == 0 {
            return Ok(0);
        }

        // 2. Process packets with memory prefetch hints for better cache performance
        for i in 0..count {
            let data = unsafe { self.umem.data(&descs[i]) };

            // Memory prefetch hint for next packet (if available)
            if i + 1 < count {
                let next_data = unsafe { self.umem.data(&descs[i + 1]) };
                unsafe { core::arch::x86_64::_mm_prefetch(next_data.as_ptr() as *const i8, core::arch::x86_64::_MM_HINT_T0) };
            }

            f(&*data);
        }

        // 3. Release to Fill queue in bulk for better efficiency
        unsafe { self.fq.produce(&descs[..count]) };

        Ok(count)
    }

    /// Legacy method for compatibility - now delegates to optimized version
    pub fn recv_batch<F>(&mut self, max_count: usize, f: F) -> Result<usize>
    where F: FnMut(&[u8])
    {
        self.recv_batch_optimized(max_count, f)
    }

    /// Optimized send batch with larger buffers and better memory management
    /// Prepare and push packets to the TX queue with enhanced performance optimizations
    pub fn send_batch_optimized<F>(&mut self, max_count: usize, mut f: F) -> Result<usize>
    where F: FnMut(&mut [u8]) -> Option<usize>
    {
        use xsk_rs::umem::frame::FrameDesc;

        // 1. Recycle completed frames (larger batch for efficiency)
        let mut completed_descs = [FrameDesc::default(); 32];
        let completed = unsafe { self.cq.consume(&mut completed_descs) };
        for i in 0..completed {
            self.tx_frames.push(completed_descs[i]);
        }

        // Use larger batch size for better throughput
        let count_to_send = max_count.min(32).min(self.tx_frames.len());
        if count_to_send == 0 {
            return Ok(0);
        }

        let mut descs = [FrameDesc::default(); 32];
        let mut actual_count = 0;

        // Process packets with optimized memory access
        for _i in 0..count_to_send {
            if let Some(mut frame) = self.tx_frames.pop() {
                let mut data = unsafe { self.umem.data_mut(&mut frame) };

                if let Some(len) = f(&mut *data) {
                    // Optimized length setting with direct memory access
                    unsafe {
                        let desc_ptr = &mut frame as *mut FrameDesc;
                        // Set length in the XDP descriptor (offset 8 in the descriptor)
                        let len_ptr = (desc_ptr as *mut u8).add(8) as *mut u32;
                        *len_ptr = len as u32;
                    }
                    descs[actual_count] = frame;
                    actual_count += 1;
                } else {
                    // Return unused frame to pool
                    self.tx_frames.push(frame);
                }
            }
        }

        // 2. Bulk produce to TX queue for maximum efficiency
        if actual_count > 0 {
            Ok(unsafe { self.tx_q.produce(&descs[..actual_count]) })
        } else {
            Ok(0)
        }
    }

    /// Legacy method for compatibility - now delegates to optimized version
    pub fn send_batch<F>(&mut self, max_count: usize, f: F) -> Result<usize>
    where F: FnMut(&mut [u8]) -> Option<usize>
    {
        self.send_batch_optimized(max_count, f)
    }

    /// Push packets already in memory (fallback or small buffers)
    pub fn send_burst(&mut self, packets: &[&[u8]]) -> Result<usize> {
        self.send_batch(packets.len(), |buf| {
            if packets.is_empty() { return None; }
            // This is still a copy, but it's the single unavoidable copy between Umems
            let p = packets[0]; // Simplified for now as we take one at a time in the current loop
            let len = p.len().min(buf.len());
            buf[..len].copy_from_slice(&p[..len]);
            Some(len)
        })
    }

    /// Get the raw FD for waiting with tokio
    pub fn fd(&self) -> i32 {
        self.tx_q.fd().as_raw_fd()
    }
}
